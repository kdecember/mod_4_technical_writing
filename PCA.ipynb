{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is essentially a method of preprocessing data, for the purpose of increasing generalizability of our model. PCA reduces the dimensionality of the data by reducing the features down to common vectors. The goal of this method is to reduce overfitness by ensuring our model does not measure the same feature characteristic multiple times.\n",
    "\n",
    "For example, think of a housing price model. The home has many characteristics; number of bedrooms, number of bathrooms, square footage, lot size, etc. Many of these features can be considered proxies of the size of the home, larger lot sizes can lead to larger home square footage, which leads to more bathrooms, bedrooms, etc. Having the model predict price based on the aforementioned features can reduce generalizability because the model is being overtuned to certain patterns present in the training data.\n",
    "\n",
    "Let's take a look at how we can apply PCA to a dataset; SKlearn thankfully makes this rather simple. The data set we will be working with is on Cars and their respective Miles Per Gallon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean training data data\n",
    "cars = pd.read_csv('cars.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(cars.drop('mpg', axis=1),\n",
    "                                                    cars['mpg'],\n",
    "                                                   random_state=20)\n",
    "\n",
    "X_train[' cubicinches'].replace(' ', np.nan, inplace=True)\n",
    "X_train[' cubicinches'] = X_train[' cubicinches'].map(float)\n",
    "X_train[' cubicinches'].fillna(X_train[' cubicinches'].mean(skipna=True), inplace=True)\n",
    "\n",
    "X_train[' weightlbs'].replace(' ', np.nan, inplace=True)\n",
    "X_train[' weightlbs'] = X_train[' weightlbs'].map(float)\n",
    "X_train[' weightlbs'].fillna(X_train[' weightlbs'].mean(), inplace=True)\n",
    "b\n",
    "X_train[' cylinders'] = X_train[' cylinders'].map(float)\n",
    "X_train[' hp'] = X_train[' hp'].map(float)\n",
    "X_train[' time-to-60'] = X_train[' time-to-60'].map(float)\n",
    "X_train[' year'] = X_train[' year'].map(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale training data\n",
    "\n",
    "ss=StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train.drop(' brand', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning & scale test data\n",
    "\n",
    "def clean(df):\n",
    "    for col in [' cubicinches', ' weightlbs']:\n",
    "        df[col].replace(' ', np.nan, inplace=True)\n",
    "        df[col] = df[col].map(float)\n",
    "        df[col].replace(np.nan, df[col].mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def to_float(df):\n",
    "    for col in [' cylinders', ' hp', ' time-to-60', ' year']:\n",
    "        df[col] = df[col].map(float)\n",
    "    return df\n",
    "\n",
    "def drop(df):\n",
    "    return df.drop(' brand', axis=1)\n",
    "\n",
    "def scale(df):\n",
    "    return ss.transform(df)\n",
    "\n",
    "test_cleaned = clean(X_test)\n",
    "test_floated = to_float(test_cleaned)\n",
    "test_dropped = drop(test_floated)\n",
    "test_scaled = scale(test_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7504046085411239"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1=LinearRegression()\n",
    "lr1.fit(X_train_sc, y_train)\n",
    "lr1.score(test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate PCA\n",
    "\n",
    "pca = PCA(n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit & transform PCA to train data\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test data\n",
    "\n",
    "test_pca = pca.transform(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7525161654890221"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_pca, y_train)\n",
    "lr.score(test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see that the regression with PCA performed better on the testing data. This is because the model was fitted to training data that had been processed through a PCA algorithm, which allowed the model to generalize more effectively. \n",
    "\n",
    "Remember, this is in relation to the TEST data. The performance on the training data will actually be lower for PCA models. This is because it's level of overfitness was reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
