{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is the process by which bodies of text are analyzed in order to predict a given feature of the text. For instance, if I had 10 different books, and wanted to determine which of them were science fiction, then I would run the contents of the books through an NLP algorithm to create a dataframe of numerical values associated with each word in each book. These numerical values are then used to predict the target. \n",
    "\n",
    "The metric of interest in this context is the TFIDF, which is the Term Frequency multiplied by the Inverse Document Frequency. In other words, TFIDF is the product of multiplying the frequency of the term in one document, by the rarity of the word across all documents.\n",
    "\n",
    "Generally speaking, we don't want to run this analysis on every word. Some words will appear frequently in all documents and won't provide us with any predictive information; words such as \"the\" or \"it\". We get rid of these words using a stop_word method. We also dont wan't to run the analysis on the entirety of the word either, but instead it's root; words such as \"running\" can be reduced to \"run\".\n",
    "\n",
    "Let's take a look at this process in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import string, re\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "url_a = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/A.txt\"\n",
    "url_b = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/D.txt\"\n",
    "article_a = urllib.request.urlopen(url_a).read()\n",
    "article_a_st = article_a.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### search for patterns in the data to pull out the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "arta_tokens_raw = nltk.regexp_tokenize(article_a_st, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### move all letters to lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arta_tokens = [i.lower() for i in arta_tokens_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign stop words to a variable\n",
    "\n",
    "nltk.corpus.stopwords.words(\"english\")\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter out words in the stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arta_tokens_stopped = [w for w in arta_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initiate a stemmer and stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this acts to find the roots of the words, or thereabouts\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "arta_stemmed = [stemmer.stem(word) for word in arta_tokens_stopped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join all the words in the list into a single string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_a = ' '.join(arta_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perform tfidf analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "response = tfidf.fit_transform([cleaned_a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pass into DataFrame for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstain</th>\n",
       "      <th>achiev</th>\n",
       "      <th>action</th>\n",
       "      <th>adopt</th>\n",
       "      <th>affair</th>\n",
       "      <th>amazon</th>\n",
       "      <th>back</th>\n",
       "      <th>base</th>\n",
       "      <th>bring</th>\n",
       "      <th>busi</th>\n",
       "      <th>...</th>\n",
       "      <th>two</th>\n",
       "      <th>union</th>\n",
       "      <th>us</th>\n",
       "      <th>use</th>\n",
       "      <th>vocal</th>\n",
       "      <th>vote</th>\n",
       "      <th>welcom</th>\n",
       "      <th>without</th>\n",
       "      <th>word</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.104257</td>\n",
       "      <td>0.104257</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104257</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.156386</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.156386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    abstain    achiev    action     adopt    affair    amazon      back  \\\n",
       "0  0.052129  0.052129  0.052129  0.052129  0.052129  0.052129  0.104257   \n",
       "\n",
       "       base     bring      busi  ...       two     union        us       use  \\\n",
       "0  0.104257  0.052129  0.052129  ...  0.104257  0.052129  0.156386  0.052129   \n",
       "\n",
       "      vocal      vote    welcom   without      word     would  \n",
       "0  0.052129  0.052129  0.052129  0.052129  0.052129  0.156386  \n",
       "\n",
       "[1 rows x 127 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlpskl = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())\n",
    "nlpskl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have successfully transformed the text of the document into numerical values with which we can begin modeling. Because this example only has 1 document, and because we don't have a target to predict, we can't perform any actual modeling. Don't worry though, the modeling process is relatively simple compared to the data wrangling performed in this walkthrough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
